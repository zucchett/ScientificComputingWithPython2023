{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy\n",
    "\n",
    "In this lecture, we will start to explore some practical problems that can be solved by applying algorithms to some data. Python provides several libraries to perform complex scientific and technical operations, and [SciPy](https://scipy.github.io/devdocs/index.html) is one of the most complete available libraries.\n",
    "\n",
    "SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.\n",
    "\n",
    "Available sub-packages include:\n",
    "\n",
    "- `constants`: physical constants and conversion factors\n",
    "- `linalg`: linear algebra routines (today)\n",
    "- `optimize`: optimization algorithms including linear programming (next lecture)\n",
    "- `interpolate`: interpolation tools (next lecture)\n",
    "- `stats`: statistical functions (next lectures)\n",
    "- `integrate`: numerical integration routines (next lecture)\n",
    "- `fft`: Discrete Fourier Transform algorithms (next lectures)\n",
    "- `fftpack`: Legacy interface for Discrete Fourier Transforms (next lectures)\n",
    "- `sparse`: sparse matrices and related algorithms\n",
    "- and many others\n",
    "\n",
    "The `scipy` is the core package for scientific routines in Python; it is meant to operate efficiently on Numpy arrays, so that Numpy and `scipy` work hand in hand.\n",
    "\n",
    "In this noteboook, we will review a few examples and applications. Sometimes Numpy implements those methods too: if a given algorithm is present both in Numpy and Scipy, typically the latter is more performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as la\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations\n",
    "\n",
    "Remember that two vectors may have two very different products:\n",
    " - The [inner product](https://numpy.org/doc/stable/reference/generated/numpy.inner.html), expressed as $v^T w$ (with $v$ and $w$ that are column vectors), is the product of a $1 \\times n$ vector with an $n \\times 1$ vector, and the result of the inner product is a **scalar**.\n",
    " - The [outer product](https://numpy.org/doc/stable/reference/generated/numpy.outer.html) of two vectors is given by:\n",
    "$$\n",
    "v\\otimes w = vw^T\n",
    "$$\n",
    "and the result of the outer product is a **matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm of a vector\n",
    "v = np.array([1, 2])\n",
    "print(\"Norm:\", la.norm(v))\n",
    "\n",
    "# distance between two vectors, represented by numpy arrays\n",
    "w = np.array([1, 1])\n",
    "print(\"Distance:\", la.norm(v - w))\n",
    "\n",
    "# inner product\n",
    "print(\"Dot prod:\", v.dot(w))\n",
    "\n",
    "# inner product\n",
    "print(\"Inner prod:\", np.inner(v, w)) # with 1D arrays, np.dot and np.inner are equivalent\n",
    "\n",
    "# outer product\n",
    "print(\"Outer prod:\\n\", np.outer(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, `scipy` also offers optimized methods to calculate the trace of a matrix (the sum of the diagonal elements in a square matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.random.randint(100, size=(6, 6))\n",
    "print(M, '\\n')\n",
    "print(\"Trace:\\n\", M.trace(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the determinant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Determinant:\\n\", la.det(M), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the inverse (if exists):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inverse of M:\\n\", la.inv(M), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: calculate the covariance matrix with the outer product\n",
    "\n",
    "As a first practical example for matrix operations, we want to calculate the [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix) from a given data set.\n",
    "\n",
    "Suppose we have $n$ measurements (rows) and $p$ variables (columns). The dataset would be a matrix $V$, which  is therefore a $n \\times p$ matrix. If all the variables (columns) have **zero mean**, the covariance matrix can be evaluated as:\n",
    "\n",
    "$$C = \\frac{V^T V}{n-1}$$\n",
    "\n",
    "This is a multiplication of a $p \\times n$ matrix ($V^T$), and a $n \\times p$ matrix ($V$), which results in a $p \\times p$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 4 sequences of 10 random numbers (flat between 0 and 1)\n",
    "n, p = 10, 4\n",
    "v = np.random.random((n, p))\n",
    "print(v) # 10 x 4 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean of each sequence (row) with a numpy reduction\n",
    "v_mean = np.mean(v, axis=0)\n",
    "print(\"Mean for each column:\\n\", v_mean, '\\n')\n",
    "\n",
    "# re-center each sequence (column) around its mean\n",
    "w = v - v_mean\n",
    "print(\"Centered matrix:\\n\", w, '\\n')\n",
    "\n",
    "# compute the covariance matrix\n",
    "cov = (w.T).dot(w) / (n - 1)\n",
    "print(\"Covariance matrix:\\n\", cov, '\\n')\n",
    "\n",
    "# The covariance matrix can be calculated directly from numpy\n",
    "np_cov = np.cov(v, rowvar=False)\n",
    "# remember to specify rowvar=False is you want to use this notation!\n",
    "print(\"Covariance matrix with Numpy:\\n\", np_cov, '\\n')\n",
    "\n",
    "print(\"Are they the same?\", np.allclose(cov, np_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear systems\n",
    "\n",
    "In several cases, some computational problems can be reduced to solving linear systems.\n",
    "If you are dealing with a problem that can be expressed with a linear systems of the type:\n",
    "\n",
    "$$A x = b$$\n",
    "\n",
    "Of course, $A$ needs to be a **square** matrix.\n",
    "\n",
    "`scipy` provides a `solve()` [method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html) for solving it.\n",
    "\n",
    "Still, knowing a little bit what are the algorithms underneath comes handy sometimes, e.g. the solve method can be instructed about what kind of matrix $A$ is likely to be (symmetric, hermitian, positive definite, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 2, 0], [1, -1, 0], [0, 5, 1]])\n",
    "b = np.array([2, 4, -1])\n",
    "print(\"A:\\n\", A, '\\n')\n",
    "print(\"b:\\n\", b, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let Scipy find the x solutions\n",
    "x = la.solve(A, b)\n",
    "print(\"x:\", x, '\\n')\n",
    "print(\"Ax:\", np.dot(A, x), '\\n')\n",
    "print(\"Is Ax == b?\", np.allclose(np.dot(A, x), b)) # note: do not use == with floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Decomposition\n",
    "\n",
    "Unfortunately, if the amount of input data is large, solving linear systems by inverting the matrix can quickly become unpractical from the computational point of view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000 # now try with 100000...\n",
    "A = np.random.randn(n, n)\n",
    "b = np.random.randn(n)\n",
    "x = la.solve(A, b)\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example is the [Netflix Competition](https://en.wikipedia.org/wiki/Netflix_Prize), where a matrix of $400000 \\times 18000$ (ratings times movies) needed to be dealt with.\n",
    "\n",
    "In these cases, the problem should be tackled by operating some convenient transformation of the input matrix.\n",
    "\n",
    "In the mathematical discipline of linear algebra, a **matrix decomposition** is a factorization of a matrix into a product of matrices. Matrix decompositions are an important step in solving linear systems in a computationally efficient manner.\n",
    "\n",
    "### Lower-Upper factorization\n",
    "\n",
    "Let $A$ be a **square** matrix. An [LU factorization](https://en.wikipedia.org/wiki/LU_decomposition) refers to the factorization of $A$, with a proper row and/or column orderings or permutations, into two factors: a lower triangular matrix $L$ and an upper triangular matrix $U$:\n",
    "\n",
    "$A=LU$\n",
    "\n",
    "when solving a system of linear equations, $Ax=b=LUx$, the solution is done in two logical steps:\n",
    "\n",
    "1. solve $Ly=b$ for $y$.\n",
    "2. solve $Ux=y$ for $x$.\n",
    "\n",
    "Often a permutation $P$ is needed (*partial pivoting*) to best reorder the rows of the original matrix.\n",
    "\n",
    "The `scipy.linalg` method `lu()` performs the [LU decomposition](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 3, 4], [2, 1, 3], [4, 1, 2]])\n",
    "print(\"A:\\n\", A, \"\\n\")\n",
    "\n",
    "P, L, U = la.lu(A)\n",
    "# P is the permutation matrix of the original matrix A\n",
    "print(\"P:\\n\", P, '\\n')\n",
    "print(\"L:\\n\", L, '\\n')\n",
    "print(\"U:\\n\", U, '\\n')\n",
    "print(\"LU:\\n\", np.dot(L, U), '\\n')\n",
    "plu = np.dot(P, np.dot(L, U))\n",
    "print(\"PLU:\\n\", plu, '\\n')\n",
    "print(\"Are PLU and A the same?\", np.allclose(plu, A), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "\n",
    "Given a **square** $n\\times n$ matrix $A$, with $\\det{A}\\ne0$, then there exist $n$ linearly independent eigenvectors and $A$ may be decomposed in the following manner:\n",
    "\n",
    "$$ \n",
    "A=V\\Lambda V^{-1}\n",
    "$$\n",
    "\n",
    "where $\\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$ and the columns of $V$ are the corresponding eigenvectors of $A$.\n",
    "\n",
    "Eigenvalues are roots of the *characteristic polynomial* of $A$:\n",
    "\n",
    "$$\n",
    "\\det{(A-\\lambda I)}=0\n",
    "$$\n",
    "\n",
    "with $I$ being the identity matrix.\n",
    "\n",
    "Eigenvectors are the vectors that solve the equation:\n",
    "\n",
    "$$\n",
    "A v = \\lambda v\n",
    "$$\n",
    "\n",
    "for all eigenvalues $\\lambda$.\n",
    "\n",
    "The eigendecomposition is performed by the method `eig()` ([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0, 1, 1], [2, 1, 0], [3, 4, 5]])\n",
    "print(\"A:\\n\", A, '\\n')\n",
    "\n",
    "l, V = la.eig(A)\n",
    "# the eigenvalues\n",
    "print(\"l:\\n\", l, '\\n')\n",
    "print(\"real(l):\\n\", np.real_if_close(l), '\\n')\n",
    "# V is the matrix of the eigenvectors\n",
    "print(\"V:\\n\", V, '\\n')\n",
    "print(\"A v:\", A.dot(V[:, 0]), \" = l v:\", V[:, 0]*np.real_if_close(l[0])) # test the definition of the first eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the decomposition $V \\Lambda V^{-1}$ returns the original matrix $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.dot(V, np.dot(np.diag(np.real_if_close(l)), la.inv(V)))\n",
    "print(D, '\\n')\n",
    "print(\"Are A and D equal?\", np.allclose(A, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "Another important matrix decomposition is the *singular value decomposition* or SVD. For any $m\\times n$ matrix $A$, we may write:\n",
    "\n",
    "$$A=USV^T$$\n",
    "\n",
    "where (assuming $A$ is real):\n",
    " - $U$ is a $m\\times m$ orthogonal, unitary matrix\n",
    " - $S$ (spectrum) is a rectangular, diagonal $m\\times n$ matrix with diagonal entries $d_1,\\dots,d_m$ all non-negative\n",
    " - $V$ is a $n\\times n$ orthogonal, unitary matrix.\n",
    "\n",
    "The singular-value decomposition is a generalization of the eigendecomposition in the sense that it can be applied to any $m \\times n$ matrix whereas eigenvalue decomposition can only be applied to diagonalizable matrices. \n",
    "\n",
    "Given an SVD of $A$, as described above, the following holds:\n",
    "\n",
    "$$\n",
    "A^T A = VS^TU^T USV^T = VS^TSV^T \n",
    "$$\n",
    "$$\n",
    "A A^T = US^TV^T VSU^T = US^TSU^T \n",
    "$$\n",
    "\n",
    "The right-hand sides of these relations describe the **eigenvalue decompositions** of the left-hand sides. Therefore, the SVD can be seen as running twice the eigenvalue decomposition for the equations above.\n",
    "\n",
    "Consequently:\n",
    "* the columns of V (right-singular vectors) are eigenvectors of $A^TA$.\n",
    "* the columns of U (left-singular vectors) are eigenvectors of $AA^T$.\n",
    "* the elements on the diagonal of the matrix $S^TS$ are the eigenvalues of $A^TA$ or $AA^T$ (follows from the eigenvalue decomposition $A = V \\Lambda V^{-1}$). In other words, the elements of the diagonal matrix $S$ are the square root of the eigenvalues of $A^TA$ or $AA^T$.\n",
    "\n",
    "A geometrical representation of SVD is given by the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/Singular-Value-Decomposition.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say, the SVD is [implemented](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) in `scipy.linalg` in the the `svd()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 5, 4\n",
    "A = np.random.randn(m, n) #+ 1.j*np.random.randn(m, n) # it also work with complex numbers\n",
    "print(\"A:\\n\", A, '\\n')\n",
    "\n",
    "# perform the SVD\n",
    "U, s, Vt = la.svd(A)\n",
    "\n",
    "# attention: s is the vector of the diagonal termns, not a matrix\n",
    "print(\"shapes: U =\", U.shape, \"S:\", s.shape, \"V^T:\", Vt.shape, '\\n')\n",
    "print(\"Spectrum:\\n\", s, '\\n')\n",
    "print(\"U:\\n\", U, '\\n')\n",
    "print(\"V^T:\\n\", Vt, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the definition of SVD by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the matrix S\n",
    "S = np.zeros((m, n))\n",
    "for i in range(min(m, n)):\n",
    "    S[i, i] = s[i]\n",
    "print(\"S:\\n\", S, '\\n')\n",
    "\n",
    "SVD = np.dot(U, np.dot(S, Vt))\n",
    "print(\"SVD:\\n\", SVD, '\\n')\n",
    "print(\"Are the SVD and A matrices equal?\", np.allclose(SVD, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis \n",
    "\n",
    "Principal Components Analysis (PCA) aims to find and rank all the eigenvalues and eigenvectors of the covariance matrix of a given dataset. This is useful because high-dimensional data (with $p$ features) may have nearly all their variation in a small number of dimensions $k < p$, i.e. in the subspace spanned by the eigenvectors of the covariance matrix that have the $k$ largest eigenvalues. If we project the original data into this subspace, we can have a dimension reduction (from $p$ to $k$) with a limited loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dataset with a skewed 2D distribution\n",
    "mu = [0, 0] # centered on 0\n",
    "cv = [[0.6, 0.2], [0.2, 0.2]] # input covariance matrix: asymmetric sigmas\n",
    "n = 1000\n",
    "# generate random numbers according to a 2D normal distribution\n",
    "X = np.random.multivariate_normal(mu, cv, n)#.T\n",
    "print(\"Shape:\", X.shape, '\\n', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lt.scatter(X[0,:], X[1,:], alpha=0.2)\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.2)\n",
    "\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerically, PCA can be done either by means of **eigendecomposition on the covariance matrix** or via **SVD on the data matrix**. Even though the latter is usually preferred, let's have first a look at the former.\n",
    "\n",
    "\n",
    "### PCA with eigendecomposition\n",
    "\n",
    "Let's recall the definition of the covariance matrix (of 2 variables):\n",
    "\n",
    "$$\n",
    "{\\rm Cov}(X,Y)=\\frac{\\sum_{i=1}^n (X_i-\\bar{X})(Y_i-\\bar{Y})}{n-1}\n",
    "$$\n",
    "\n",
    "with Cov$(X,X)$ the variance of the variable $X$, and $n$ the number of entries.\n",
    "\n",
    "In the case the features of the datasets have all **zero mean**, the covariance matrix is of the form:\n",
    "\n",
    "$$\n",
    "{\\rm Cov}(X)=\\frac{X^T X}{n-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the covariance matrix\n",
    "cov = np.cov(X, rowvar=False)\n",
    "#cov = np.dot(X.T, X)/(n-1) # this would yield the same result\n",
    "print(\"Covariance matrix:\\n\", cov, '\\n')\n",
    "\n",
    "# now find the eigenvectors of the covariance matrix\n",
    "l, V = la.eig(cov)\n",
    "# take only the real component, if possible\n",
    "l = np.real_if_close(l)\n",
    "\n",
    "print(\"Eigenvalues:\\n\", l, '\\n')\n",
    "print(\"Eigenvectors:\\n\", V, '\\n')\n",
    "\n",
    "# Check that V is actually an orthogonal matrix (and thus its transpose is also its inverse)\n",
    "#print(\"VV^T:\\n\", V.dot(V.T), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphically represent the eigenvectors rescaled by the eigenvalues\n",
    "plt.figure(figsize=(8, 8))\n",
    "# the original data distribution\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "\n",
    "# a scale factor to graphically enhance the lines - only for visualization purposes\n",
    "scale_factor = 3\n",
    "\n",
    "# draw each eigenvectors rescaled by the eigenvalues\n",
    "for li, vi in zip(l, V.T):\n",
    "    #print(\"Eigenvalue:\", li, \",\\teigenvector:\", vi)\n",
    "    # the line is defined by specifying its beginning and its end \n",
    "    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]], 'r-', lw=2)\n",
    "\n",
    "# fix the size of the axes to have the right visual effect\n",
    "plt.axis([-3, 3, -3, 3])\n",
    "plt.title('Eigendecomposition:\\neigenvectors of covariance matrix scaled by eigenvalue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the eigenvectors and eigenvalues to rotate the data, i.e. take the eigenvectors as new basis vectors and redefine the data points w.r.t. this new basis:\n",
    "\n",
    "$$X' = X \\cdot V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate all the data points accordingly to the new base by multiplying by the matrix V\n",
    "Xp = np.dot(X, V)\n",
    "\n",
    "# then plot the rotated dataset and its \"axes\"\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(Xp[:, 0], Xp[:, 1], alpha=0.2)\n",
    "# same eigenvalues as before, assume we rotated properly the data\n",
    "for li, vi in zip(l, np.dot(V.T, V)): # np.dot(V.T, V) is equivalent to np.diag([1]*len(l))\n",
    "    plt.plot([0, scale_factor*li*vi[0]], [0, scale_factor*li*vi[1]], 'r-', lw=2)\n",
    "plt.axis([-3, 3, -3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we only use the first column of `Xp`, we will have the projection of the data onto the first principal component, capturing the majority of the variance in the data using only a single feature that is a linear combination of the original features.\n",
    "\n",
    "We may need to transform the (reduced) data set to the original feature coordinates for interpreation. This is simply another linear transform (matrix multiplication):\n",
    "\n",
    "$$X'' = X' \\cdot V^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate back the data to the original basis, this time by multiplying by the matrix V^T, the opposite as before\n",
    "Xpp = np.dot(Xp, V.T)\n",
    "\n",
    "# re-plot in the original basis\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(Xpp[:, 0], Xpp[:, 1], alpha=0.2)\n",
    "for li, vi in zip(l, V.T):\n",
    "    plt.plot([0, scale_factor*li*vi[0]], [0, scale_factor*li*vi[1]], 'r-', lw=2)\n",
    "plt.axis([-3, 3, -3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction via PCA\n",
    "\n",
    "Given the spectral decomposition:\n",
    "\n",
    "$$ \n",
    "A=V\\Lambda V^{-1}\n",
    "$$\n",
    "\n",
    "with $\\Lambda$ of rank $p$. Reducing the dimensionality to $k<p$ simply means setting to zero all but the first $k$ diagonal values (ordered from the largest to the smaller in module; that is the default in numpy/scipy).\n",
    "\n",
    "In this way we catch the most relevant part of its variability (covariance).\n",
    "\n",
    "Since the trace is invariant under change of basis, the total variability is also unchaged by PCA. By keeping only the first $k$ principal components, we can still retain $\\sum_1^k \\lambda_i/\\sum_1^p \\lambda_i$ of the total variability.\n",
    "\n",
    "In general, the degree of dimension reduction is specified as keeping enough principal components so that a certain fraction (say 90%) of the total variability is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, let's consider only the component that exhibits the largest variability, and discard the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l, V = np.linalg.eig(cov)\n",
    "Lambda = np.diag(l)\n",
    "print(\"Lambda:\\n\", Lambda, '\\n')\n",
    "print(\"Trace(A):\\n\", cov.trace(), '\\n')\n",
    "print(\"Trace(Lambda):\\n\", Lambda.trace(), '\\n')\n",
    "\n",
    "print(\"By selecting the component 0, we retain %.2f%% of the total variability\" % (Lambda[0, 0]/Lambda.trace()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with SVD\n",
    "\n",
    "We saw that SVD is a decomposition of the data matrix $X=UDV^T$ where $U$ and $V$ are orthogonal matrices and $D$ is a diagnonal matrix.\n",
    "\n",
    "Compared with the eigendecomposition of a matrix $A=W\\Lambda W^{−1}$, we see that SVD gives us the eigendecomposition of the matrix $XX^T$, which as we have just seen, is basically a scaled version of the covariance for a data matrix with zero mean, with the eigenvectors given by $V^T$ and eigenvalues by $D^2 / (n-1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD decomposition on the dataset\n",
    "U, S, Vt = np.linalg.svd(X)\n",
    "\n",
    "# Rescale the SVD spectrum to get the eigenvalues\n",
    "l_svd = S**2/(n-1)\n",
    "# The matrix Vt already contains the eigenvectors\n",
    "\n",
    "print(\"U:\\n\", U, '\\n')\n",
    "print(\"S:\\n\", S, '\\n')\n",
    "print(\"Vt:\\n\", Vt, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Eigenvalues\n",
    "print(\"Eigendecomposition:\\n\", l)\n",
    "print(\"SVD:\\n\", l_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare Eigenvectors\n",
    "print(\"Eigendecomposition:\\n\", V)\n",
    "print(\"SVD:\\n\", Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "# draw each eigenvectors rescaled by the eigenvalues\n",
    "for li, vi in zip(l_svd, Vt):\n",
    "    #print(\"Eigenvalue:\", li, \",\\teigenvector:\", vi)\n",
    "    # the line is defined by specifying its beginning and its end \n",
    "    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]], 'r-', lw=2)\n",
    "plt.axis([-3, 3, -3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same as before, and rotate the data to the new basis of the eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate all the data points accordingly to the new base by multiplying by the transpose of matrix V\n",
    "Xp_svd = np.dot(X, Vt.T)\n",
    "\n",
    "# then plot the rotated dataset and its \"axes\"\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(Xp_svd[:, 0], Xp_svd[:, 1], alpha=0.2)\n",
    "\n",
    "# draw each eigenvectors rescaled by the eigenvalues\n",
    "for li, vi in zip(l_svd, np.dot(Vt, Vt.T)):\n",
    "    #print(\"Eigenvalue:\", li, \",\\teigenvector:\", vi)\n",
    "    # the line is defined by specifying its beginning and its end \n",
    "    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]], 'r-', lw=2)\n",
    "\n",
    "plt.axis([-3, 3, -3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Complexity\n",
    "\n",
    "Profiling (e.g. with `timeit`) doesn’t tell us much about how an algorithm will perform on a different computer since it is determined by the hardware features. To compare performance in a device-indpendent fashion, a formalism (a.k.a the \"Big-O\") is used that characterizes functions in terms of their rates of growth as a function of the size *n* of the input.\n",
    "\n",
    "An algorithm is compared to a given function $g(n)$ with a well defined scaling with *n*, e.g. $n^2$; if the ratio of the two is bounded, than that algorithm is ${\\cal O}(g(n))$. Note that:\n",
    "* only the largest terms in the scaling of $g(n)$ is kept in the notation\n",
    "* two algorithms can have the same complexity and have very different performance; the same complexity only implies that the difference in performance is independent of *n*.\n",
    "\n",
    "### Comparing bubble sort ${\\cal O}(n^2)$ and merge sort ${\\cal O}(n\\log{n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(n, k):\n",
    "    return k * n * n\n",
    "\n",
    "def f2(n, k):\n",
    "    return k * n * np.log(n)\n",
    "\n",
    "n = np.arange(0.1, 20001)\n",
    "plt.plot(n, f1(n, 1), c='blue')\n",
    "plt.plot(n, f2(n, 1000), c='red')\n",
    "plt.xlabel('Size of input (n)', fontsize=16)\n",
    "plt.ylabel('Number of operations', fontsize=16)\n",
    "plt.legend(['$\\mathcal{O}(n^2)$', '$\\mathcal{O}(n \\log n)$'], loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://wiki.python.org/moin/TimeComplexity) for the complexity of operations on standard Python data structures. Note for instance that searching a list is much more expensive than searching a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Complexity\n",
    "\n",
    "We can also use the ${\\cal O}$ notation in the same way to measure the space complexity of an algorithm.  The notion of space complexity becomes important when your data volume is of the same magnitude or larger than the memory you have available. In that case, an algorihtm with high space complexity may end up having to swap memory constantly, and will perform far worse than its time complexity would suggest.\n",
    "\n",
    "Just as you should have a good idea of how your algorithm will scale with increasing *n*, you should also be able to know how much memory your data structures will require. For example, if you had an $n\\times p$ matrix of integers, an $n\\times p$ matrix of floats, and an $n\\times p$ matrix of complex floats, how large can $n$ and $p$ be before you run out of RAM to store them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how much overhead Python objects have:\n",
    "# a raw integer should be 64 bits or 8 bytes only\n",
    "\n",
    "import sys\n",
    "print(\"int:\", sys.getsizeof(1), \"bytes\") # bytes\n",
    "print(\"long:\", sys.getsizeof(1234567890123456789012345678901234567890), \"bytes\")\n",
    "print(\"float:\", sys.getsizeof(3.14), \"bytes\")\n",
    "print(\"complex:\", sys.getsizeof(3j), \"bytes\")\n",
    "print(\"char:\", sys.getsizeof('a'), \"bytes\")\n",
    "print(\"string:\", sys.getsizeof('hello world'), \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"100x100 bool:\", np.ones((100, 100), dtype='bool').nbytes, \"bytes\")\n",
    "print(\"100x100 int:\", np.ones((100, 100), dtype='int').nbytes, \"bytes\") # default is 64 bits or 8 bytes\n",
    "print(\"100x100 float:\", np.ones((100, 100), dtype='float').nbytes, \"bytes\") # default is 64 bits or 8 bytes\n",
    "print(\"100x100 complex:\", np.ones((100, 100), dtype='complex').nbytes, \"bytes\")\n",
    "print(\"10kx10k float:\", np.ones((10000, 10000), dtype='float').nbytes / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
