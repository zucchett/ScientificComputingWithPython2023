{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Vodafone users' fluxes\n",
    "\n",
    "The study of the flux of people inside urban areas is of paramount importance to achieve an optimal understanding of emerging critical issues in the local mobility, and to explore areas of potential improvements in the infrastructures and local transports.\n",
    "\n",
    "The mobility of users within and toward Padova has been monitored using the data provided by the Vodafone mobile carrier, which provides the information based on the users' connections to the network cells.\n",
    "The data provided by the carrier encompasses the monitoring of the users connected to the Vodafone network in Padova in a four-month period from February to May of 2018.\n",
    "\n",
    "To provide statistical insights on the number and the flow of users, the data is aggregated based on the origin and movements of the users by averaging the number of connections during the time of the monitoring.\n",
    "\n",
    "To further avoid privacy violation issues, all observations with less than 30 units (e.g. day-areas for which $<$ 30 users have contributed) have been discarded and/or merged into dedicated categories (indicated with \"altro\", or \"other\").\n",
    "\n",
    "\n",
    "## Datasets \n",
    "\n",
    "The data is provided in `.csv` files.\n",
    "\n",
    "* __day_od.csv__: table of the origins and destinations of the users averaged by the day of the week. The data is provided with details of the month, type of user (resident in Padova/Italian visitor/foreign visitor), country of provenance, together with the province and comune of the user (if available).\n",
    "* __distinct_users_day.csv__: table of the number of distinct users by origin. The data is provided with details of the month, type of user (resident in Padova/Italian visitor/foreign visitor), country of provenance, together with the province and comune of the user (if available).\n",
    "\n",
    "The information is stored in the fields according to the following scheme: \n",
    "\n",
    "- __MONTH__: month analyzed\n",
    "- __DOW__: day analyzed\n",
    "- __ORIGIN__: users' origin area (do not consider this field)\n",
    "- __DESTINATION__: users' destination area (do not consider this field)\n",
    "- __CUST_CLASS__: user type (resident / Italian visitor / foreigner visitor)\n",
    "- __COD_COUNTRY__: users' country code (e.g. 222=Italy)\n",
    "- __COD_PRO__: users' province code (e.g. 12=Varese) \n",
    "- __PRO_COM__: users' comune code (e.g. 12026=Busto Arsizio)\n",
    "- __FLOW__: number of movements for given date-time (with a minimum of 30 users)\n",
    "- __VISITORS__: overall number of users \n",
    "\n",
    "Together with the data files, three lookup-tables are provided to allow matching the Italian institute of STATistics (ISTAT) country, province and comune codes to the actual names.\n",
    "\n",
    "* __codici_istat_comune.csv__: lookup file containing the mapping between _comune_ ISTAT code-names\n",
    "* __codici_istat_provincia.csv__: lookup file containing the mapping between _province_ ISTAT code-names\n",
    "* __codici_nazioni.csv__: lookup file containing mapping the _country_ code to its name\n",
    "\n",
    "Additional information, useful for the study of the flow of users, as the number of inhabitants of each province and the distance between Padova and all other Italian provinces can be extracted based on the data collected by the ISTAT:\n",
    "\n",
    "   - English: https://www.istat.it/en/analysis-and-products/databases, Italian: https://www.istat.it/it/dati-analisi-e-prodotti/banche-dati\n",
    "   \n",
    "   - English/Italian: https://www.istat.it/en/archive/157423, Italian: https://www.istat.it/it/archivio/157423\n",
    "   \n",
    "   - `.zip` package containing the distances between comuni in Veneto region: http://www.istat.it/storage/cartografia/matrici_distanze/Veneto.zip\n",
    "\n",
    "If deemed useful, the open repository [https://github.com/openpolis/geojson-italy](https://github.com/openpolis/geojson-italy) contains a `.json` file with the geographical coordinates of the provences and comuni of Italy.\n",
    "\n",
    "\n",
    "## Assignments\n",
    "\n",
    "1. Data preparation: the csv files are originated from different sources, hence resulting in differences in the encoding and end-of-lines that have to be taken into account in the data preparation phase. Make sure each .csv file is properly interpreted.\n",
    "\n",
    "   1.1 Ranking of visitors from foreign countries: based on the number of total visitors per each country, create a ranked plot of the first 20 countries with the most visitors\n",
    "   \n",
    "   1.2 Ranking of Italian visitors by province, weighted by the number of inhabitants: based on the number of total visitors per Italian province, create a ranked plot of the first 20 provinces with the most visitors taking into account the number of inhabitants.\n",
    "\n",
    "\n",
    "2. Study of the visitors' fluxes: you are asked to provide indications on how to invest resources to improve the mobility towards Padova. Consider the three main directions of visitors and commuters getting to Padova through the main highways (from south, A13 towards Bologna-Roma; from west, A4 towards Milano-Torino; from north-east, A4 towards Venice-Trieste). Evaluate which of the three directions has to be prioritized.\n",
    "\n",
    "   2.1 Consider a simplified case involving only the mid-range mobility, based on the number of visitors/commuters from the nearby regions only\n",
    "   \n",
    "   2.2 Consider the provinces located on the three directions that are mostly contributing to the flow of weekend visitors and working daily commuters by performing a more detailed study of the fluxes based on the day of the week. Use the data available to provide what you believe is the best possible answer.\n",
    "\n",
    "\n",
    "3. Plot the distribution of the number of visitors by the distance of the province of origin. Determine which kind of function should be used to describe the distribution.\n",
    "\n",
    "   3.1 Assuming an analytic form can be used to describe the trend, create a regression or a fit to estimate the expected number of visitors by the distance of the province of origin and the corresponding uncertainties. Illustrate the difference between the resulting regression with respect to the numbers provided by the Vodafone monitoring, and highlight the five most striking discrepancies from the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet #Used to detect the encoding of the CSV files\n",
    "import codecs  #Used to read the CSV UTF-16\n",
    "import io      #Used to write the CSV ISO-8859-1\n",
    "import pandas as pd #Used to store data into dataframes\n",
    "import matplotlib.pyplot as plt #Used to represent data\n",
    "import numpy as np #Used to rename the column of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of csv files\n",
    "filename_codici_istat_comuni = \"data\\codici_istat_comune.csv\"\n",
    "filename_codici_istat_provincia = \"data\\codici_istat_provincia.csv\"\n",
    "filename_codici_nazioni = \"data\\codici_nazioni.csv\"\n",
    "filename_day_od = \"data\\day_od.csv\"\n",
    "filename_distinct_user_day = \"data\\distinct_users_day.csv\"\n",
    "filename_distance_to_pd = \"data\\R05_PD.csv\"\n",
    "filename_distance_to_pd_txt = \"data\\R05_PD.txt\"\n",
    "\n",
    "#Creating a list to boost performances of the loops\n",
    "filenames = [filename_codici_istat_comuni, filename_codici_istat_provincia, filename_codici_nazioni,\n",
    "             filename_day_od, filename_distinct_user_day, filename_distance_to_pd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function which returns the encoding of each csv file\n",
    "def check_encoding(file):\n",
    "    #Read the file\n",
    "    with open(file, 'rb') as f:\n",
    "        #Detect the encoding\n",
    "        result = chardet.detect(f.read())\n",
    "\n",
    "    #Return a list of the encodings\n",
    "    return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function wich converts the UTF-16 encoded files into ISO-8859-1 encoded files\n",
    "def encoding_converter(files):\n",
    "    for file in files:\n",
    "        #Saving the encoding of each file\n",
    "        encodings = check_encoding(file)\n",
    "\n",
    "        # If the encoding is different to ISO-8859-1 it has to be converted\n",
    "        if encodings == 'ascii' :\n",
    "            # Open the file and saving the content\n",
    "            with codecs.open(file, 'r', 'ascii') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            # Overwrite the file with a new encoding\n",
    "            with io.open(file, 'w', encoding='utf-8') as f:\n",
    "                f.write(data)\n",
    "\n",
    "        if encodings == 'utf-16':\n",
    "            # Open the file and saving the content\n",
    "            with codecs.open(file, 'r', 'utf-16') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            # Overwrite the file with a new encoding\n",
    "            with io.open(file, 'w', encoding='latin1') as f:\n",
    "                f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Ranking of visitors from foreign countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ranking of visitors from Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Study of the visitors' fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visitors_fluxes(file_customers, dow_study=False):\n",
    "    '''\n",
    "    Consider the three main directions:\n",
    "    A13 Roma-Bologna\n",
    "    A4 Torino-Milano\n",
    "    A4 Trieste-Venezia\n",
    "    '''\n",
    "    #Consider the istat province codes\n",
    "    codes = pd.read_csv(filename_codici_istat_provincia, encoding='latin1', usecols=['PROVINCIA', 'COD_PRO'])\n",
    "\n",
    "    #Consider the codes of those six provinces\n",
    "    codes = codes.loc[(codes['PROVINCIA'] == 'Torino') |\n",
    "                        (codes['PROVINCIA'] == 'Roma') |\n",
    "                        (codes['PROVINCIA'] == 'Venezia') |\n",
    "                        (codes['PROVINCIA'] == 'Bologna') |\n",
    "                        (codes['PROVINCIA'] == 'Trieste') |\n",
    "                        (codes['PROVINCIA'] == 'Milano')]\n",
    "    \n",
    "    if dow_study:\n",
    "        #Now take the data of the customers and see what's their origin and their destination\n",
    "        customers_data = pd.read_csv(file_customers, encoding='latin1', usecols = ['COD_PRO', 'FLOW'])\n",
    "    else:\n",
    "        customers_data = pd.read_csv(file_customers, encoding='latin1', usecols = ['COD_PRO','FLOW', 'DOW'])\n",
    "    \n",
    "    #Join between customers_data and province_codes in this way filters the province_codes in which we're not interested\n",
    "    data = pd.merge(customers_data, codes, on='COD_PRO')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visitors(data, range_of_mobility=\"\"):\n",
    "    #Counting the flows ov visitors from every city we're interested (both ways)\n",
    "    data = data.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "\n",
    "    far = True\n",
    "\n",
    "    if range_of_mobility == \"nearby\":\n",
    "        far = False\n",
    "\n",
    "    #Saving the informations per highway\n",
    "    A13_Roma_Bologna = far * data.FLOW.loc['Roma'] + data.FLOW.loc['Bologna']\n",
    "    A4_Milano_Torino = data.FLOW.loc['Milano'] + far * data.FLOW.loc['Torino']\n",
    "    A4_Venezia_Trieste = data.FLOW.loc['Venezia'] + data.FLOW.loc['Trieste']\n",
    "\n",
    "    highway = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "    #Labels for the plot\n",
    "    highway_labels = [\"A13 Roma-Bologna\", \"A4 Milano-Torino\", \"A4 Venezia-Trieste\"]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(highway_labels, highway, color='skyblue')\n",
    "    plt.xlabel('Highways')\n",
    "    plt.ylabel('Visitors'' flow')\n",
    "    plt.title('Visitors'' flow per highway')\n",
    "    # Set the scale to avoid the exponential notation \n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visitors(visitors_fluxes(filename_day_od))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b Mid-range mobility\n",
    "Mobility to/from nearby regions, which are Lombardia, Trentino Alto Adige, Friuli Venezia Giulia, Emilia Romagna.\n",
    "QUESTA SOLUZIONE TIENE CONTO DELLE AUTOSTRADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visitors(visitors_fluxes(filename_day_od), \"nearby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Week flow\n",
    "\n",
    "Consider the provinces located on the three directions that are mostly contributing to the flow of weekend visitors and working daily commuters by performing a more detailed study of the fluxes based on the day of the week. Use the data available to provide what you believe is the best possible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data1, data2, label):\n",
    "    #Saving the informations per highway\n",
    "    A13_Roma_Bologna = data1.FLOW.loc['Roma'] + data1.FLOW.loc['Bologna']\n",
    "    A4_Milano_Torino = data1.FLOW.loc['Milano'] + data1.FLOW.loc['Torino']\n",
    "    A4_Venezia_Trieste = data1.FLOW.loc['Venezia'] + data1.FLOW.loc['Trieste']\n",
    "\n",
    "    highway1 = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "    #Saving the informations per highway\n",
    "    A13_Roma_Bologna = data2.FLOW.loc['Roma'] + data2.FLOW.loc['Bologna']\n",
    "    A4_Milano_Torino = data2.FLOW.loc['Milano'] + data2.FLOW.loc['Torino']\n",
    "    A4_Venezia_Trieste = data2.FLOW.loc['Venezia'] + data2.FLOW.loc['Trieste']\n",
    "\n",
    "    highway2 = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "    #Labels for the plot\n",
    "    highway_labels = [\"A13 Roma-Bologna\", \"A4 Milano-Torino\", \"A4 Venezia-Trieste\"]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    '''\n",
    "    plt.bar(highway_labels, highway, color='skyblue')\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('Visitors'' flow')\n",
    "    plt.title('Visitors'' flow in ' + label)\n",
    "    # Set the scale to avoid the exponential notation \n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.tight_layout()\n",
    "    '''\n",
    "    # Plot dei valori\n",
    "    plt.plot(highway_labels, highway1, marker='o')  # 'marker' specifica il tipo di marker per i punti\n",
    "    plt.plot(highway_labels, highway2, marker='x')  # 'marker' specifica il tipo di marker per i punti\n",
    "    plt.xlabel('X')  # Etichetta asse x\n",
    "    plt.ylabel('Y')  # Etichetta asse y\n",
    "    plt.title('Plot tipo funzione dei valori di un DataFrame')  # Titolo del grafico\n",
    "    plt.grid(False)  # Abilita la griglia\n",
    "\n",
    "    plt.show()\n",
    "    #Fare istogramma grouped bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seleziono i turisti per le tre direzioni, poi li suddivido in weekend e working days\n",
    "'''\n",
    "def dow_visitors_fluxes(data):\n",
    "    data_weekend = data.loc[(data['DOW'] == \"Domenica\") | (data['DOW'] == \"Sabato\")]\n",
    "\n",
    "    data_working_day = data.loc[~data.index.isin(data_weekend.index)]\n",
    "\n",
    "    # #Join between customers_data and province_codes in this way filters the province_codes in which we're not interested\n",
    "    # data_weekend = pd.merge(weekend_flow, data, on='COD_PRO')\n",
    "    # data_working_day = pd.merge(working_day_flow, data, on='COD_PRO')\n",
    "\n",
    "    #Counting the flows ov visitors from every city we're interested (both ways)\n",
    "    data_weekend = data_weekend.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "    data_working_day = data_working_day.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "\n",
    "    plotter(data_weekend, data_working_day, \"Weekend\")\n",
    "    # plotter(data_working_day, \"Working \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_visitors_fluxes(visitors_fluxes(filename_day_od))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "Plot the distribution of the number of visitors by the distance of the province of origin. Determine which kind of function should be used to describe the distribution.\n",
    "\n",
    "Ã¨ scritto visitor quindi tolgo i non visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_visitors_distance(province_data):\n",
    "    distances_data = pd.read_csv(province_data, sep=\"\\t\", encoding=\"UTF-8\", usecols=['DEST_PROCOM', 'KM_TOT'])\n",
    "\n",
    "    distances_data.rename(columns={'DEST_PROCOM': 'PRO_COM'}, inplace=True)\n",
    "\n",
    "    distances_data['KM_TOT'] = distances_data['KM_TOT'].str.replace(',','.')\n",
    "    distances_data['KM_TOT'] = pd.to_numeric(distances_data['KM_TOT'])\n",
    "\n",
    "    distances_data = distances_data.groupby(['PRO_COM'])[['KM_TOT']].mean()\n",
    "\n",
    "    customers_data = pd.read_csv(filename_distinct_user_day, encoding=\"latin1\", usecols=['PRO_COM','VISITORS','CUST_CLASS'])\n",
    "\n",
    "    # Escludo Padova\n",
    "    customers_data['PRO_COM'] = np.where(customers_data['PRO_COM'] == 28060.0 , np.nan,\n",
    "                                np.where(customers_data['CUST_CLASS'] != 'visitor', np.nan, customers_data['PRO_COM']))\n",
    "    customers_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Elimina la colonna \"CUST_CLASS\"\n",
    "    customers_data = customers_data.drop('CUST_CLASS', axis=1)\n",
    "\n",
    "    customers_data = customers_data.groupby(['PRO_COM'])[['VISITORS']].sum()\n",
    "\n",
    "    data = pd.merge(distances_data, customers_data, on='PRO_COM')\n",
    "\n",
    "    data = data.sort_values(['VISITORS'], ascending=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_visitors_distance(filename_distance_to_pd_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visitors_distance(data, zoom=1):\n",
    "    # zoom serve a raggruppare le distanze (50 --> raggruppo le distanze ogni 50 km)\n",
    "    # calcolo il range di raggruppamento\n",
    "    period = int(data['KM_TOT'].max() / zoom)\n",
    "    somma_visitatori = []\n",
    "    for n_group in range(period):\n",
    "        data_grouped = data[data['KM_TOT'].between((n_group-1)*zoom +1 , n_group * zoom)]\n",
    "        somma_visitatori.append(data_grouped['VISITORS'].sum())\n",
    "    # somma_visitatori.pop(0)\n",
    "    \n",
    "    data_to_plot = pd.DataFrame({'KM_TOT': [km * zoom for km in range(0, period)], 'VISITORS': somma_visitatori})\n",
    "\n",
    "    # data_to_plot['KM_TOT'] = np.where(data_to_plot['KM_TOT'] == 0 , np.nan, data_to_plot['KM_TOT'])\n",
    "    # data_to_plot.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return data_to_plot\n",
    "\n",
    "    # data_nearby = data.loc[(data['KM_TOT'] < 50)]\n",
    "\n",
    "    # data_far = data.loc[~data.index.isin(data_nearby.index)]\n",
    "\n",
    "    # # Scatterplot customization based on your preferences\n",
    "    # plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "    # plt.scatter(data['KM_TOT'], data['VISITORS'], color='blue', alpha=0.7, s=50)  # Adjust markers, transparency, etc.\n",
    "    # plt.xlabel('Total Distance (KM)')\n",
    "    # plt.ylabel('Total Visitors')\n",
    "    # plt.title('Scatterplot of Distance vs. Visitors')\n",
    "    # plt.grid(True, linestyle='--', linewidth=0.5)  # Optional grid\n",
    "    # plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "    # # Optional customization based on relationships or data properties\n",
    "    # # ... (e.g., color-coding by regions, highlighting certain points)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    #Da riguardare il grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_visitors_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_visitors_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_distance_to_pd_txt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[75], line 11\u001b[0m, in \u001b[0;36mplot_visitors_distance\u001b[1;34m(data, zoom)\u001b[0m\n\u001b[0;32m      8\u001b[0m     somma_visitatori\u001b[38;5;241m.\u001b[39mappend(data_grouped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVISITORS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      9\u001b[0m somma_visitatori\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m data_to_plot \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKM_TOT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mkm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVISITORS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msomma_visitatori\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# data_to_plot['KM_TOT'] = np.where(data_to_plot['KM_TOT'] == 0 , np.nan, data_to_plot['KM_TOT'])\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# data_to_plot.dropna(axis=0, inplace=True)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_to_plot\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    730\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    731\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    732\u001b[0m     )\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "plot_visitors_distance(data_visitors_distance(filename_distance_to_pd_txt), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 \n",
    "Assuming an analytic form can be used to describe the trend, create a regression or a fit to estimate the expected number of visitors by the distance of the province of origin and the corresponding uncertainties. Illustrate the difference between the resulting regression with respect to the numbers provided by the Vodafone monitoring, and highlight the five most striking discrepancies from the expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Build linear regression model using TV and Radio as predictors\n",
    "# Split data into predictors X and output Y\n",
    "X = np.array(data['KM_TOT']).reshape(-1,1)\n",
    "y = data['VISITORS']\n",
    "\n",
    "# Initialise and fit model\n",
    "lm = LinearRegression()\n",
    "model = lm.fit(X, y)\n",
    "# plot for residual error\n",
    " \n",
    "   \n",
    "plt.scatter(X,y,color='red')\n",
    "plt.plot(X,model.predict(X),color='green')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
