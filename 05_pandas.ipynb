{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "The Numpy library is excellent for numerical computations, but it lacks support to handle missing data or non-omogeneous arrays. The **Pandas** library is based on Numpy and extends the Numpy functionality, and is currently one of the most widely used tools for data manipulation, providing high-performance, easy-to-use data structures and advanced data analysis tools.\n",
    "\n",
    "In particular Pandas features:\n",
    "\n",
    "* A fast and efficient `Series` and `DataFrame` objects for data manipulation with integrated indexing;\n",
    "* Tools for reading and writing data between in-memory data structures and different formats (CSV, Excel, SQL, HDF5);\n",
    "* Smart data alignment and integrated handling of missing data;\n",
    "* Time series-functionalities;\n",
    "* Convenient label-based slicing, fancy indexing, and subsetting of large data sets;\n",
    "* Hierarchical axis indexing provides an intuitive way of working with high-dimensional data in a lower-dimensional data structure;\n",
    "* Aggregating and transforming data with a powerful \"group-by\" engine; \n",
    "* High performance merging and joining of data sets;\n",
    "* Highly optimized for performance, with critical code paths written in Cython or C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # standard naming convention\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series\n",
    "\n",
    "Pandas Series represent an extension of the Numpy 1D arrays. The content of a Series is equivalent to a Numpy array, and in addition the axis  is labeled. Labels don't need to be unique, but must be of a *hashable* type.\n",
    "\n",
    "Since the content is of type `ndarray`, the content has to be *omogeneous*. However, there is the possibility to store heterogeneous data, but the content in this case would be of type `object`.\n",
    "\n",
    "One of the most important examples are the **time series**, which are used to keep track of the time evolution of a certain quantity.\n",
    "\n",
    "Link to the official Pandas Series [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "\n",
    "# Calling the Series constructor\n",
    "# Constructor requires the data, and optionally the indices and data type\n",
    "sr = pd.Series(np.arange(10)*0.5, index=tuple(letters[:10]), dtype=float)\n",
    "print(\"series:\\n\", sr, '\\n')\n",
    "print(\"series type:\\n\", type(sr), '\\n')\n",
    "print(\"indices:\\n\", sr.index, '\\n')\n",
    "print(\"values:\", sr.values, type(sr.values), '\\n') # values of the Series are actually a numpy array\n",
    "print(\"type:\\n\", sr.dtype, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"element by index:\", sr['f'], '\\n') # Accessing elements like arrays\n",
    "print(\"element by attribute:\", sr.f, '\\n') # Accessing elements like attributes - not recommended\n",
    "\n",
    "# selecting a subset of the series\n",
    "subsr = sr[['d', 'f', 'h']] # note the double square brackets\n",
    "print(\"series subset:\\n\", subsr, type(subsr), '\\n') # Multiple indexing returns another series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting elements and operations are the same as numpy array (slicing)\n",
    "print(sr[:3], '\\n')\n",
    "print(sr[7:], '\\n')\n",
    "print(sr[::3], '\\n')\n",
    "\n",
    "# Fancy indexing works on Series, too\n",
    "print(sr[sr > 3], '\\n')\n",
    "\n",
    "# You can also pass Series to numpy funtions\n",
    "print(np.exp(sr), '\\n')\n",
    "print(\"Series mean:\", np.mean(sr), \", std:\", np.std(sr), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series may contain non-omogeneous data; in this case, the data type is referred to as `object`. Non-homogeneous data is normally handeled also by Pandas and does not represent a problem, however this pays the price of less time-efficient operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series can be created from a python dictionary, too\n",
    "# Note that the elements can be of different types\n",
    "d = {'b' : 1, 'a' : 'cat', 'c' : [2, 3]}\n",
    "so = pd.Series(d) # alternative constructor that taks a dict as the only input\n",
    "print(so, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference between Pandas Series and Numpy arrays is that operations between Series **automatically align the data based on the label**.\n",
    "\n",
    "Thus, you can write operations without considering whether the Series involved have the same labels, or even the same size.\n",
    "\n",
    "If there is no matching element, the resulting value would be a `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(5), index=tuple(letters[:5]))\n",
    "print(\"series:\\n\", s, '\\n')\n",
    "\n",
    "s1 = s[1:]\n",
    "print(\"shifted series:\\n\", s1, '\\n')\n",
    "\n",
    "s2 = s1 + s\n",
    "print(\"shifted sum:\\n\", s2, '\\n')\n",
    "\n",
    "s3 = s1 + s[:-1]\n",
    "print(\"double shifted sum:\\n\", s3, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series\n",
    "\n",
    "**Datetime**\n",
    "\n",
    "When dealing with time, Python provides the `datetime` library that allows to store the date and time in an dedicated object, which possess several methods to access the relevant quantities (day, month, year, hours, minutes, seconds, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To define a date, the datetime module is very useful\n",
    "import datetime as dt\n",
    "\n",
    "date = dt.date.today()\n",
    "print(\"Today's date:\", date)\n",
    "\n",
    "# specify year, month, day, hour, minutes, seconds, and microseconds\n",
    "date = dt.datetime(2020, 11, 12, 10, 45, 10, 15)\n",
    "print(\"Date and time:\", date)\n",
    "print(\"Month:\", date.month, \"and minutes:\", date.minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas Timestamps**\n",
    "\n",
    "Timestamped data is the most basic type of time series data that associates values with points in time.\n",
    "\n",
    "Functions like `pd.to_datetime` can be used to convert between different formats and, for instance, when reading the time stored as a string from a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the timestamp, which is the nanoseconds from January 1st 1970 (Unix time)\n",
    "tstamp = pd.Timestamp(date)\n",
    "#tstamp = pd.Timestamp(dt.datetime(1970, 1, 1, 0, 0, 0, 1))\n",
    "print(\"Timestamp:\", tstamp.value)\n",
    "\n",
    "# when creating a timestamp the format can be explicitly passed\n",
    "ts = pd.to_datetime('2010/11/12', format='%Y/%m/%d')\n",
    "print(\"Time:\", ts, \", timestamp:\", ts.value, \", type:\", type(ts))\n",
    "\n",
    "ts = pd.to_datetime('12-11-2010 10:39', format='%d-%m-%Y %H:%M')\n",
    "print(\"Time:\", ts, \", timestamp:\", ts.value, \", type:\", type(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas Date range**\n",
    "\n",
    "Time series are very often used to describe the behaviour of a quantity as a function of time. Pandas has a special type of index for that, `DatetimeIndex`, that can be created e.g. with the function `pd.data_range()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DatetimeIndex using ranges:\n",
    "days = pd.date_range(date, periods=7, freq='D')\n",
    "print(\"7 days range:\", days)\n",
    "\n",
    "seconds = pd.date_range(date, periods=3600, freq='s')\n",
    "print(\"1 hour in seconds:\", seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about the frequency strings, please check the [documentation](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard series can be created, and (a range of) elements can be used as indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"index:\\n\", days, '\\n')\n",
    "tseries = pd.Series(np.random.normal(10, 1, len(days)), index=days)\n",
    "print(\"time series:\\n\", days, '\\n')\n",
    "# Extracting elements\n",
    "print(\"slice by position:\\n\", tseries[0:4], '\\n')\n",
    "print(\"slice by date range:\\n\", tseries['2020-9-11' : '2020-9-14'], '\\n') # note that includes end time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.to_datetime` can also be used to create a `DatetimeIndex` if the argument is a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.to_datetime([1, 2, 3, 4], unit='D', origin=pd.Timestamp('1980-02-03')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "A pandas DataFrame can be thought as a tabular spreadsheet, although the performance, the functionalities and the capabilities are very different.\n",
    "\n",
    "Similarly to Series, the DataFrame structure also contains labeled axes (rows and columns). Arithmetic operations **align on both row and column labels**. Each column in a DataFrame is a Series object: as a matter of fact, a DataFrame can be thought of as a dict-like container for Series objects.\n",
    "\n",
    "The elements can be of all types, and missing data could be present too (represented as `NaN`).\n",
    "\n",
    "For future reference (or for people already familiar with R), a pandas DataFrame is also similar to the R DataFrame.\n",
    "\n",
    "Link to the official [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructor\n",
    "\n",
    "A DataFrame objects can be created by passing a dictionary of objects. Note that the dictionary values are not omogeneous and do not have the same length. In these cases, pandas will automatically adjust the sizes, by replicating the content or adding NaN if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'A' : 1.,\n",
    "    'B' : pd.Timestamp('20130102'),\n",
    "    'C' : pd.Series(3, index=range(4), dtype='float32'),\n",
    "    'D' : np.arange(7, 11),\n",
    "    'E' : pd.Categorical([\"test\", \"train\", \"test\", \"train\"]), # a Series that represents a category label\n",
    "})\n",
    "# the keys of the dictionary represent the labels of the columns\n",
    "# since no index is specified, the simplest one [0, 1, 2, ...] is added by Pandas automatically\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of DataFrame with a `DatatimeIndex` object as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = 10\n",
    "columns = ['A', 'B', 'C', 'D']\n",
    "dates = pd.date_range('11/9/2020 14:45:00', freq='h', periods=entries) # days/month/year\n",
    "df = pd.DataFrame(np.random.randn(entries, len(columns)), index=dates, columns=columns)\n",
    "df # pay attention that the date is printed as year-day-month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"C\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection\n",
    "\n",
    "#### Slicing\n",
    "\n",
    "DataFrame slicing allows to select a subset of the DataFrame, or an entire column (a Series):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard and safe\n",
    "print(df['A'], '\\n', type(df['A']), '\\n') # Returns a Series (a column)\n",
    "\n",
    "# equivalent but dangerous (imagine blank spaces in the name of the column, or a column named \"T\")\n",
    "print(df.A, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy-like slicing by row ranges is possible, and usually returns a **view** of the original DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting rows by range. Returns another DataFrame (usually a view)\n",
    "print(df[0:3], '\\n')\n",
    "\n",
    "# or by index range\n",
    "print(df[\"2020-11-09 14:45:00\" : \"2020-11-09 16:45:00\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection by label\n",
    "\n",
    "The most common way to select elements, rows, or columns in a DataFrame is by using the `.loc[]` method.\n",
    "\n",
    "`.loc` supports multi-indexing, and usually returns a **copy** of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a part of the DataFrame (in this case, a row)) using a label. Returns a Series\n",
    "dfs = df.loc[dates[0]] # equivalent to df.loc[df.index[0]]\n",
    "print(dfs, '\\n', type(dfs), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting on a multi-axis by label:\n",
    "dfa = df.loc[:, ['A','B']]\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing label slicing, both endpoints are included:\n",
    "df.loc['2020-11-09 18:45:00':'2020-11-09 20:45:00', ['A','B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting an individual element\n",
    "print(df.loc[dates[1], 'A'], '\\n', type(df.loc[dates[1], 'A']), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.at()` method is equivalent to `.loc[]`. Use `at` if you only need to get or set a single value in a DataFrame or Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.at[dates[1], 'A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting by position\n",
    "\n",
    "`.iloc[]` is similar to `.loc[]`, but instead of labels, it uses pure integer-location based indexing for selection by position.\n",
    "\n",
    "But differently from `.loc[]`, `.iloc[]` usually returns a **view**, not a copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select via the position of the passed integers:\n",
    "print(df.iloc[3], '\\n', type(df.iloc[3]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you specify just one axis or idex, a Series is returned. If you specify both axis or indices, you get a DataFrame instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row and column ranges selected with numpy-like notation:\n",
    "dfv = df.iloc[3:5, 0:2]\n",
    "print(dfv, '\\n', type(df.iloc[3:5, 0:2]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of multiple elements with lists\n",
    "df.iloc[[1, 2, 4], [0, 2]] # selecting rows 1,2 and 4 for columns 0 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing rows explicitly\n",
    "df.iloc[1:3, :]\n",
    "\n",
    "# slicing columns explicitly\n",
    "df.iloc[:, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary to `.loc[]` and `.at[]`, there is also `.iat[]` alongside `.iloc[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting an individual element by position: no difference between iloc and iat\n",
    "print(df.iloc[1,1], \", type:\", type(df.iloc[1,1]))\n",
    "print(df.iat[1,1], \", type:\", type(df.iat[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masks\n",
    "\n",
    "Boolean masks can be used in the same way as Numpy, and they represent a very powerful way of filtering out data with certain features. Just like Numpy fancy indexing, using a mask usually returns a **copy** of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting on the basis of boolean conditions applied to the whole DataFrame\n",
    "dfc = df[df > 0]\n",
    "dfc.iat[0, 0] = -99\n",
    "# a DataFrame with the same shape is returned, with NaN's where condition is not met\n",
    "# Note that when a NaN is present in a column of integers, the column (Series) is casted to float\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by a boolean condition on the values of a single column\n",
    "mask = dfc['B'] < 0.5\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only the rows that correspond to a True in the Series used as mask\n",
    "dfc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Queries**\n",
    "\n",
    "Pandas uses a database-like engine to select elements according to a query on the columns of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq = df.query('C > 0.5')\n",
    "dfq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is equivalent to `dfq = df[df['C'] > 0.5]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = df[df['C'] > 0.5]\n",
    "dfw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy and views in DataFrames\n",
    "\n",
    "The view/copy behaviour in Pandas is not as easy as it may appear, as there are counter-intuitive exceptions. There was a plan to fix this by quite some time, but a fix has not been deployed yet.\n",
    "\n",
    "Check this discussion [here](https://www.practicaldatascience.org/html/views_and_copies_in_pandas.html):\n",
    "\n",
    "    In numpy, the rules for when you get views and when you don’t are a little complicated, but they are consistent: certain behaviors (like simple indexing) will always return a view, and others (fancy indexing) will never return a view.\n",
    "\n",
    "    But in pandas, whether you get a view or not—and whether changes made to a view will propagate back to the original DataFrame—depends on the structure and data types in the original DataFrame.\n",
    "\n",
    "\n",
    "In summary, there is only one way to write safe code when dealing with slides of a dataframe: after every instruction that selects a subset of a DataFrame, force the copy by appending `.copy()` to the slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignement\n",
    "\n",
    "Assignment is typically performed after a selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to copy the DataFrame if you plan to modify it, and you don't want to change the original object\n",
    "dfa = df.copy()\n",
    "\n",
    "# setting values by label (same as by position)\n",
    "dfa.at[dates[0], 'A'] = -1\n",
    "\n",
    "# setting and assigning a numpy array\n",
    "dfa['D'] = np.array([5] * len(dfa))\n",
    "\n",
    "# defining a brand new column by means of a pd.Series: indexes must be the same!\n",
    "dfa['E'] = pd.Series(np.arange(len(dfa))*2, index=dfa.index)\n",
    "\n",
    "# using masks for assigment\n",
    "dfa[dfa < 0] = -dfa\n",
    "\n",
    "dfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping\n",
    "\n",
    "Dropping columns is an example of **a method that does not modify the original object**, and returns a new modified object. In other words, if you want to keep the modified DataFrame, perform a new assignment:\n",
    "\n",
    "```python\n",
    "df = df.drop(...)\n",
    "```\n",
    "Alternatively, the modification of the original object can be forced by specifying `inplace=True` among the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = dfa.copy()\n",
    "\n",
    "# Dropping by column..\n",
    "dfb.drop(['E'], axis=1)\n",
    "dfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is no effect on the original object. That's because  new object is returned instead. To keep it, there are two alternatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = dfb.drop(columns=['E'])\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb.drop(columns=['E'], inplace=True) # equivalent to the previous one, but the original object has been replace inplace\n",
    "dfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n",
    "\n",
    "Pandas primarily uses the value `np.nan` to represent missing data. It is by default not included in computations. If there is a `NaN` entry in a Series of integers, the type of the Series will be changed to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wNan = dfb[dfb > 0.5]\n",
    "df_wNan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with at least a Nan\n",
    "df_wNan.dropna(how='any')\n",
    "df_wNan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a mask\n",
    "df_wNan.isna()\n",
    "# df_wNan.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing data (not recommended, unless you really mean it)\n",
    "df_wNan.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "\n",
    "Operations on the elements of a DataFrame are quite straightforward, as the syntax is the same as the one used for Series. Also for DataFrames, operations are performed between elements that share the same labels. Operations on columns are extremly fast, almost as fast as the actual operation between elements in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistics (mean() just as an example)\n",
    "# on rows\n",
    "print(df.mean(axis=0), '\\n')\n",
    "# on columns\n",
    "print(df.mean(axis=1), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global operations on columns\n",
    "df.apply(np.sum) # or whatever function defined by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also lambda functions are accepted\n",
    "df.apply(lambda x: x - x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax is as usual similar to that of numpy arrays\n",
    "df['S'] = df['A'] + df['C']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of a function: apply vs transform\n",
    "\n",
    "User-defined or standard functions can be applied on entire DataFrames or columns, with very short execution times.\n",
    "\n",
    "There are two main methods, `apply()` and `transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcos(theta):\n",
    "    theta = theta * (np.pi / 180)\n",
    "    return np.cos(theta)\n",
    "\n",
    "# Apply method with custom function\n",
    "dfa['cosine'] = dfa[\"E\"].apply(dcos)\n",
    "\n",
    "# Transform method with lambda function\n",
    "dfa['EplusOne'] = dfa[\"E\"].transform(lambda x: x + 1)\n",
    "dfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major differences between `apply` and `transform` are:\n",
    "\n",
    "   - Input: `apply` passes all the columns to the custom function, while `transform` passes each column.\n",
    "   - Output: the custom function passed to `apply` can return a scalar, or a Series or DataFrame, while the custom function passed to `transform` must return a sequence (a Series, array or list) with the same length.\n",
    "\n",
    "In summary, `transform` works on just one Series, and `apply` works on the entire DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "Pandas provides various functions for easily combining together Series and DataFrames in join / merge-type operations.\n",
    "\n",
    "**Concat**\n",
    "\n",
    "Concatenation (adding rows) is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = pd.DataFrame(np.arange(40).reshape(10, 4))\n",
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split DataFrame into 3 pieces, row-wise\n",
    "pieces = [rdf[:3], rdf[3:7], rdf[7:]]\n",
    "print(pieces, '\\n')\n",
    "pieces[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it back together\n",
    "pd.concat(pieces)\n",
    "\n",
    "# in this case, indices are already set; if they are not, indices can be ignored\n",
    "#pd.concat(pieces, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of dimension mismatch, `NaN` are added where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge/Join**\n",
    "\n",
    "SQL-like operations on table can be performed on DataFrames. This is a quite advanced use case, refer to the [doc](https://pandas.pydata.org/pandas-docs/stable/merging.html#merging) for more info/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key': ['foo', 'bar'], 'lval': [1, 2]})\n",
    "right = pd.DataFrame({'key': ['foo', 'bar'], 'rval': [4, 5]})\n",
    "\n",
    "pd.merge(left, right, on=\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "In real world applications, it's quite common that several entries (row) belong to a certain entity, or \"group\". DataFrames have a powerful tool to perform operations on entries of the same group. The method is called `.groupby()`, and it usually involves one or more of the following steps:\n",
    "\n",
    "* Splitting the data into groups based on some criteria\n",
    "* Applying a function to each group independently\n",
    "* Combining the results into a data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B' : [1, 1, 2, 3, 2, 2, 1, 3],\n",
    "                    'C' : np.arange(8),\n",
    "                    'D' : np.linspace(10, -10, 8)})\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and then applying the sum() \n",
    "# function to the resulting groups (effective only where numerical values are present)\n",
    "gdf.groupby('A').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: find maximum value in column D for each group, and assign the value to a new column\n",
    "gdf['M'] = gdf.groupby('A')['D'].transform(np.max)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-indexing\n",
    "\n",
    "Hierarchical / Multi-level indexing allows sophisticated data analysis on higher dimensional data. In practice, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1D) and DataFrames (2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat multi-dimensional index\n",
    "tuples = list(zip(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n",
    "          ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']))\n",
    "multi_index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n",
    "print(multi_index, '\\n', type(multi_index), '\\n')\n",
    "\n",
    "# Create multi-indexed dataframe or series\n",
    "s = pd.Series(np.arange(8)/np.pi, index=multi_index)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-indexing enables further features of the groupby method,\n",
    "# e.g. when group-by by multiple columns\n",
    "gdf.groupby(['A', 'B']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: a demonstration of the efficiency of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go the hard way and load a (relatively) large dataset with approximately 1 million rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download the file. Run the command just once\n",
    "#!wget https://www.dropbox.com/s/xvjzaxzz3ysphme/data_000637.txt -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./data/data_000637.txt\"\n",
    "data = pd.read_csv(file_name)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do some operations among (elements of) columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itime = dt.datetime.now()\n",
    "print(\"Begin time:\", itime)\n",
    "\n",
    "# the one-liner command\n",
    "data['WEIGHTEDSUM'] = data['TDC_CHANNEL'] * 2.1 + data['BX_COUNTER'] * 0.1 + 2\n",
    "\n",
    "ftime = dt.datetime.now()\n",
    "print(\"End time:\", ftime)\n",
    "print(\"Elapsed time:\", ftime - itime)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loop\n",
    "def conversion(data):\n",
    "    result = []\n",
    "    for i in range(len(data)): \n",
    "        result.append(data.loc[data.index[i], 'TDC_CHANNEL'] * 2.1 + data.loc[data.index[i], 'BX_COUNTER'] * 0.1 + 2)\n",
    "    return result\n",
    "\n",
    "itime = dt.datetime.now()\n",
    "print(\"Begin time:\", itime)\n",
    "data['WEIGHTEDSUM'] = conversion(data)\n",
    "ftime = dt.datetime.now()\n",
    "print(\"End time:\", ftime)\n",
    "print(\"Elapsed time:\", ftime - itime)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
