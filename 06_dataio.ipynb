{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input and Output\n",
    "\n",
    "According to a popular model, the elements of data science are:\n",
    "\n",
    "* Obtaining data\n",
    "* Scrubbing data\n",
    "* Exploring data\n",
    "* Modeling data\n",
    "* iNterpreting data\n",
    "\n",
    "and hence the acronym OSEMN, pronounced as “Awesome”.\n",
    "\n",
    "We will start with the **O**, and let's have a quick look at what it all boils down to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O: obtaining data\n",
    "#!wget https://www.dropbox.com/s/ebe1cnyd2gm836a/populations.txt -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: scrubbing data\n",
    "import numpy as np\n",
    "data = np.loadtxt('data/populations.txt')\n",
    "year, hares, lynxes, carrots = data.T # trick: columns to variables\n",
    "# this is not real scrubbing, rather a simple operation to better handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E: exploring data\n",
    "!cat data/populations.txt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.axes([0.2, 0.1, 0.5, 0.8]) \n",
    "plt.plot(year, hares, year, lynxes, year, carrots) \n",
    "plt.legend(('Hare', 'Lynx', 'Carrot'), loc=(1.05, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M: modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the data a clear (and reasonable) correlations between pray and predator becomes evident.\n",
    "\n",
    " - How can it be quantified?\n",
    " - Is that statistical significant?\n",
    " - What about the correlation between carrots and hares? And hares and lynxes?\n",
    " \n",
    " We will deal with data modeling in few weeks, when we will learn to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N: interpreting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding correlations in data is one of the main goals of data science, though that is not the end of the story: as this interesting [site](http://tylervigen.com/spurious-correlations) demonstrates, **correlations does not imply causation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and processing data\n",
    "\n",
    "Accessing data may not be as easy as it seems. The previous example, where the data is stored on your own machine, represents the simplest (but alas, less common) case. Data is usually stored on remote machines, which can be either *publicly accessible* (everyone can access the data, even without credentials) rather than *private*. In the case of the former, things may be straightforward, whereas in the latter case you need to worry about a few things.\n",
    "\n",
    "In both cases, depending on the size of the dataset, the managment of the dataset can become extremely complicated. We won't deal here with large datasets (which require a whole course *per se*), but still we should pay attention to few basic things: for instance, *it is not wise to keep (and even worse commit) data into a git repository*!\n",
    "\n",
    "The suggestion is then to create a directory somewhere and copy the example datasets there. From a terminal:\n",
    "\n",
    "```bash\n",
    "\n",
    "# create a data directory in your home directory\n",
    "mkdir data/\n",
    "\n",
    "# check the content (it's empty now of course)\n",
    "ls -ltr data/\n",
    "\n",
    "# in the case you need to move there:\n",
    "cd data/\n",
    "\n",
    "# if you need to copy a file\n",
    "cp data/data_original.txt data/data_copy.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public data\n",
    "\n",
    "An increasingly number of institutions, reaserch centers, experiments, ... are making their data public.\n",
    "\n",
    "A nice set of interesting datasets can be found on this [server](https://archive.ics.uci.edu/ml/datasets.php) that collects training/test data for machine learning developments. Several of those belong to physical sciences, and may be worth a look. Since they are public, you can freely download any of those. However, they usually come with a license (e.g. you won't be allowed to make profit from them), and deserve a reference in your paper, if you publish some result.\n",
    "\n",
    "Sometimes, they are used as the input to machine learning challenges, where different groups compete to achieve the best result.\n",
    "\n",
    "In the following, we consider a dataset from the MAGIC experiment. We will get them with the `wget` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset and its description to the data/ directory\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -P data/\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -P data/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the metadata file. This can (and should) be done from a terminal\n",
    "!cat data/magic04.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the data without downloading the file(s)\n",
    "\n",
    "It is possible to download and load remote files via their url from within python (and thus on a jupyter session). This is a rather powerful tool as it allows http communications, I/O streaming and so on.\n",
    "\n",
    "Care should be put as the dataset is stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names'\n",
    "with urllib.request.urlopen(url) as data_file:\n",
    "    #print(data_file.read(300))\n",
    "    for line in data_file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Copy data from a remote machine: SSH and SCP\n",
    "\n",
    "Often datasets are not available on websites, but rather they are stored on some remote machine. Several tools have been developed allow you to get remote data, even from within Python (e.g. [paramiko](https://www.paramiko.org/)), but it's often enough to get a local copy.\n",
    "\n",
    "The connection to a remote machine is usually performed through an **SSH** (Secore SHell) connection.\n",
    "The Secure Shell Protocol (SSH) is a cryptographic network protocol to perform remote login and command-line execution on a remote machine.\n",
    "\n",
    "The unix `scp` command is of great help in this case. It allows to copy a file from/to a remote machine using an SSH connection. Its syntax resembles the one of the `cp` command, e.g. from a terminal:\n",
    "\n",
    "```bash\n",
    "scp username@machinename.unipd.it:/path/to/the/file/filename.* path/to/the/target/\n",
    "```\n",
    "\n",
    "The `scp` works provided that you have the necessary permissions and authorization to log in to the remote machine, and you know the location of the files within that machine. This seems trivial, but it's one of the most difficult aspects you will have to deal when trying to obtain the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it could be worse. Especially in large reaserch centers or companies, the remote machine `R` is often not accessible from \"outside\", meaning that it is hidden behind a firewall. In that case you may need to use to access to a *gate* machine `G` first (with very limited disk space and functionality), and only then you can connect to the machine. This also means that you cannot run the `scp` command twice to get the data.\n",
    "\n",
    "The simplest solution is to create an `ssh` tunnel:\n",
    "\n",
    "``` bash\n",
    "ssh -L port:<address of R known to G>:22 <user at G>@<address of G> \n",
    "\n",
    "scp -P port <user at R>@localhost:/path/to/the/file file-name-to-be-copied\n",
    "```\n",
    "\n",
    "where `port` is the port number which is a number between 1025 and 65535, `R` is the machine where the files are located, `G` is the gate, and `user` is the username.\n",
    "\n",
    "The first command creates a connection to `G` and redirects your local port `port` to port 22 of `R`, so that you can connect to port 22 of `R` with `ssh` or `scp` (second command).\n",
    "It could happen that the chosen port number is already in use: in that case just pick another unused port.\n",
    "\n",
    "In summary, just getting the data could be complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formats\n",
    "\n",
    "Datasets can be stored in several different ways. Sometimes, they have formats which are application-dependent, even though more and more standards are being established.\n",
    "\n",
    "#### Proprietary (closed) formats\n",
    "\n",
    "In some of those cases, the format of the data is proprietary, meaning that the software house does not provide the documentation needed to read and write the data in that format. We won't cover those cases in this course, and there are a number of reasons to avoid them.\n",
    "\n",
    "#### Open formats\n",
    "\n",
    "In case an open, widely-accepted data formats, Python has \"readers\" for most of these, another reason for being the optimal programming language for data analysis. In the follwing, we will explore the most common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text files \n",
    "\n",
    "Plain text files are very common, and are used for \"readibility\", at the price of a poor storing efficiency. [UTF-8](https://en.wikipedia.org/wiki/UTF-8) is the most common encoding.\n",
    "\n",
    "Reading (and writing) text files in Python is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fie_name = \"data/magic04.data\"\n",
    "file = open(file_name, 'r') # opening a file in read only  mode\n",
    "str1 = file.readline()\n",
    "print(str1)\n",
    "file.close() # closing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you **always** need to explicitly close the file with `file.close()`.\n",
    "\n",
    "Pay attention to the `mode`:\n",
    "  -  `w` : the file will be opened in write only mode. If a file is opened in `w` mode and a file already exists then the existing files content will be overwritten\n",
    "  -  `r` : the file will be opened in read only mode. This is the default mode\n",
    "  -  `a` : the file will be opened in append mode and the data will be written at the end of the file\n",
    "  -  `r+` : the file will be opened in read and write mode\n",
    "  -  `b` : the file will be opened in the binary mode (see later)\n",
    "\n",
    "You can also specify more than one *mode*, for instance `rb`.\n",
    "\n",
    "An alternative syntax, that is equivalent to the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/magic04.data\"\n",
    "n = 0\n",
    "\n",
    "# mode can be specified for writing, reading or both\n",
    "with open(file_name, mode = 'r') as f:\n",
    "    # print-out the whole file\n",
    "    # print(f.read()) \n",
    "    for line in f:\n",
    "        print(n, \":\")\n",
    "        # print line by line\n",
    "        print(line)\n",
    "        # each line is a string, you need to split it yourself\n",
    "        #for c in line.split(','): print(\"  \", c) # check the functionalities of the split() method\n",
    "        n += 1\n",
    "        if n > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the file was automatically closed at the end of the indentation block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, it's possible to write files to a txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name = \"data/magic04.out\"\n",
    "\n",
    "with open(out_file_name, 'w') as outfile:\n",
    "    outfile.write(line + '\\n') # write last line of the previous example to a file\n",
    "\n",
    "!cat data/magic04.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV files\n",
    "\n",
    "Text files are already framed into a defined structured, in a \"table-like\" manner. If the values/features/attributes are separated with a comma, these files are colled \"comma separated values\" (CSV), even though the separator may well not be the `,` symbol. It could also happen that the separator is a whitespace, or a tab character.\n",
    "\n",
    "Python has a package to deal with those, named `csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "n = 0\n",
    "\n",
    "with open('data/magic04.data') as data_file:\n",
    "    for line in csv.reader(data_file, delimiter=','): # the delimiter is often guessed by the reader\n",
    "        #print(line) # all items treated as strings\n",
    "        # again note that elements of each line are treated as strings\n",
    "        # if you need to convert them into numbers, you need to to that yourself\n",
    "        fLength, fWidth, fSize, fConc, fConc1, fAsym, fM3Long, fM3Trans, fAlpha, fDist = map(float, line[:-1])\n",
    "        category = line[-1]\n",
    "        print(fLength, fWidth, fSize, fConc, fConc1, fAsym, fM3Long, fM3Trans, fAlpha, fDist, category)\n",
    "        n += 1\n",
    "        if n > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, csv files have comments (e.g. starting with `#`), which cannot be interpreted by the reader. Tricks like:\n",
    "\n",
    "```python\n",
    "csv.reader(row for row in f if not row.startswith('#'))\n",
    "```\n",
    "\n",
    "may be useful to skip those lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and writing txt and csv files with Numpy and Pandas\n",
    "\n",
    "Numpy provides the useful methdod `np.loadtxt()` to read txt files, as we have seen in the introduction.\n",
    "\n",
    "Pandas itself provides integrated and convenient tools to read and write CSV files with the `read_csv()` method of DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the unpacked version\n",
    "#!wget https://www.dropbox.com/s/ga9wi6b40cakgae/data_000637.txt -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name= \"data/data_000637.txt\"\n",
    "data = pd.read_csv(file_name, nrows=10, skiprows=range(1,1))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pandas is smart enough to determine that the first row consists of the headers of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "#!cat data/data_000637.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns can be specified when reading the file, if they do not exist. An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name = \"data/magic04.data\"\n",
    "data = pd.read_csv(file_name, nrows=1000, names=['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'category'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON files\n",
    "\n",
    "JSON stands for JavaScript Object Notation, and is a format widely used for web-based resource sharing. It is very similar in structure to a Python nested dictionary. It is humand readable (then human-editable), even if readability may be difficult for complex data structures. Its use is convenient in Python, as its items can be accessed by key. Here is an example http://json.org/example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/example.json\n",
    "{\n",
    "    \"glossary\": {\n",
    "        \"title\": \"example glossary\",\n",
    "            \"GlossDiv\": {\n",
    "            \"title\": \"S\",\n",
    "                    \"GlossList\": {\n",
    "                \"GlossEntry\": {\n",
    "                    \"ID\": \"SGML\",\n",
    "                                    \"SortAs\": \"SGML\",\n",
    "                                    \"GlossTerm\": \"Standard Generalized Markup Language\",\n",
    "                                    \"Acronym\": \"SGML\",\n",
    "                                    \"Abbrev\": \"ISO 8879:1986\",\n",
    "                                    \"GlossDef\": {\n",
    "                        \"para\": \"A meta-markup language, used to create markup languages such as DocBook.\",\n",
    "                                            \"GlossSeeAlso\": [\"GML\", \"XML\"]\n",
    "                    },\n",
    "                                    \"GlossSee\": \"markup\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/example.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python includes a dedicated module, called just `json`, that allows to import a json file as a (nested) Python dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # import the JSON module\n",
    "data = json.load(open('data/example.json'))\n",
    "print(\"Type:\", type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the (nested) dict can be parsed using standard key lookups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['glossary']['GlossDiv']['GlossList']['GlossEntry']['GlossTerm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle\n",
    "\n",
    "Sometimes, you may want to save and read more complex Python data structures than int, floats and strings, as for example dictionaries, tuples, lists, or even more complex objects.\n",
    "\n",
    "In these cases, the `pkl` format is very handful. Pickle is used for serializing and de-serializing Python object structures, also called flattening. Serialization consists of converting an object in memory to a byte stream that can be stored on disk. Later on, this character stream can then be retrieved and de-serialized back to a Python object.\n",
    "\n",
    "There are fundamental differences between the pickle protocols and JSON (JavaScript Object Notation):\n",
    "\n",
    "- JSON is a text serialization format (it outputs unicode text, although most of the time it is then encoded to utf-8), while pickle is a binary serialization format;\n",
    "\n",
    "- JSON is human-readable, while pickle is not;\n",
    "\n",
    "- JSON is interoperable and widely used outside of the Python ecosystem, while pickle is Python-specific;\n",
    "\n",
    "- JSON, by default, can only represent a subset of the Python built-in types, and no custom classes; pickle can represent an extremely large number of Python types (many of them automatically, by clever usage of Python’s introspection facilities; complex cases can be tackled by implementing specific object APIs);\n",
    "\n",
    "- Unlike pickle, deserializing untrusted JSON does not in itself create an arbitrary code execution vulnerability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# define the data structure\n",
    "ages_dict = {'Fido' : 3, 'Laika' : 16, 'Skipper' : 10, 'Balou' : 9}\n",
    "\n",
    "filename = \"data/dog_ages.pkl\"\n",
    "\n",
    "outfile = open(filename, 'wb') # note: no filename extension. 'wb' stands for w: write and b: binary\n",
    "pickle.dump(ages_dict, outfile) # \"dump\" the data to pkl\n",
    "outfile.close() # remember to close the file\n",
    "\n",
    "!ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickle\n",
    "infile = open(filename, 'rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5\n",
    "\n",
    "HDF5 is a hierarchical format often used to store complex scientific data. For instance, Matlab now saves its data to HDF5. It is particularly useful to store complex hierarchical data sets with associated metadata, for example, the results of a computer simulation experiment.\n",
    "\n",
    "The main concepts associated with HDF5 are:\n",
    "\n",
    "* file: container for hierachical data - serves as ‘root’ for tree\n",
    "* group: a node for a tree\n",
    "* dataset: array for numeric data - can be huge\n",
    "* attribute: small pieces of metadata that provide additional context\n",
    "\n",
    "Now let's create a dummy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# creating a HDF5 file\n",
    "\n",
    "filename = \"data/example.hdf5\"\n",
    "\n",
    "if os.path.exists(filename): # check if file exists, and if it does, delete it\n",
    "    os.remove(filename)\n",
    "\n",
    "with h5py.File(filename, 'w') as f: # create file\n",
    "    project = f.create_group('project') # create project\n",
    "    expt1 = project.create_group('expt1') # create group(s)\n",
    "    expt2 = project.create_group('expt2')\n",
    "    expt1.create_dataset('counts', (100,), dtype='i') # create datasets\n",
    "    expt2.create_dataset('values', (1000,), dtype='f')\n",
    "\n",
    "    expt1['counts'][:] = np.arange(100)\n",
    "    expt2['values'][:] = np.arange(1000)/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read HDF5 file\n",
    "\n",
    "with h5py.File(filename) as f:\n",
    "    project = f['project']\n",
    "    print(project['expt1']['counts'][:10])\n",
    "    print(project['expt2']['values'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLS (Excel, Google Sheets)\n",
    "\n",
    "Excel files are used to store tabular data and can have more than one sheet to store different relational tables.\n",
    "\n",
    "Excel (or Google Sheets) are incredibly powerful and common outside adademia, unfortunately, so analysts might find a file with such a format given to them for analysis.\n",
    "\n",
    "Pandas provides the `read_excel()` method to load the data from Excel files; but be aware that it requires the `xlrd` package, which sometimes is not included by default. If it's missing, run`python3 -m pip install xlrd`.\n",
    "\n",
    "Be aware that Excel files may contain multiple tabs (sheets), and you can provide to `read_excel()` the name or the index (or even a list of names or indices) of the sheet(s) you want to read.\n",
    "\n",
    "When reading an Excel file, the first column is used as row labels, and the first row is used for the column labels; but you can specify otherwise with the `header` and `index_col` optional arguments.\n",
    "\n",
    "Link to the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an xls dataset\n",
    "#!wget https://web.stanford.edu/~ashishg/msande111/excel/iris.xls -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from an excel file\n",
    "iris_xl_data = pd.read_excel(\"data/iris.xls\", sheet_name='Sheet1', header=1)\n",
    "\n",
    "# check the first few rows of data loaded from Excel\n",
    "iris_xl_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from a database\n",
    "\n",
    "While plain text, csv, json, ... are good enough to store small-size datasets, databases are designed to hold almost any size of data. Most organizations store their large amount of business-specific data in local or cloud databases such as MySQL, MongoDB, or PostgreSQL.\n",
    "\n",
    "These databases are normally created, accessed, and managed using the Structured Query Language (SQL) for different purposes. Python provides a lot of connectors to access the data from different types of databases using the same SQL queries. Again, Pandas play a key role as you can read the queried data as a dataframe and perform multiple operations of your choice.\n",
    "\n",
    "#### MySQL\n",
    "\n",
    "In order to access and use a MySQL database, you need the `mysql-connector` module:\n",
    "\n",
    "`python3 -m pip install mysql-connector`\n",
    "\n",
    "\n",
    "The following example shows how to perform a connection to the database, perform a query and read the database as a Pandas DataFrame.\n",
    "\n",
    "```python\n",
    "# import mysql dependency\n",
    "import mysql.connector\n",
    "\n",
    "# create a connection to the database\n",
    "conn = mysql.connector.connect(\n",
    "\thost=\"localhost\",\n",
    "\tuser=\"yourusername\",\n",
    "\tpassword=\"yourpassword\",\n",
    "\tdatabase=\"yourdatabase\"\n",
    ")\n",
    "\n",
    "# select data from DB table\n",
    "query = \"SELECT * FROM tablename\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# close connection\n",
    "conn.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQLite\n",
    "\n",
    "SQLite is a library that is used to implement lightweight databases that are small, fast, self-contained, and highly reliable.\n",
    "\n",
    "Python provides the `sqlite3` module that handles the interaction with SQLite databases. This module already comes preinstalled with any modern Python version.\n",
    "\n",
    "The following example will show how to download the Sakila database (a very complete database of movies rentals), open it with Python, perform a query, and import the result as Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://gist.github.com/Piyush3dB/726bf7012785d6e0fd691c3655c92654/raw/2c17ccb2eb33b3396bfa96284c53f0718a4ea62c/sakila.db -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "!curl -o \"./data/sakila.db\" \"https://gist.github.com/Piyush3dB/726bf7012785d6e0fd691c3655c92654/raw/2c17ccb2eb33b3396bfa96284c53f0718a4ea62c/sakila.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: actor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4116\\3568143819.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# query data from database: select all content from the table \"actor\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"SELECT * FROM actor\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# create a DataFrame from the DB data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: actor"
     ]
    }
   ],
   "source": [
    "# import dependency\n",
    "import sqlite3 as sql\n",
    "\n",
    "# create a connection to the database and a cursor to execute queries\n",
    "conn = sql.connect('data/sakila.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# query data from database: select all content from the table \"actor\"\n",
    "query = \"SELECT * FROM actor\"\n",
    "results = cur.execute(query).fetchall()\n",
    "\n",
    "# create a DataFrame from the DB data\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# print dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROOT\n",
    "\n",
    "[ROOT](https://root.cern.ch/) needs a special mention. It is still nowadays, and by far, the most convenient tool to store and manage complex datasets pertaining physics experiments where \"events\" are recorded, in particular High Energy, Nuclear, Astro physics.\n",
    "\n",
    "ROOT allows something that other data structures are not really optimized to do:\n",
    " - a variable number of attributes/features/variables can be set on a per-event basis\n",
    " - a nested structure, with **complex data objects (classes) and references between them**\n",
    "\n",
    "ROOT is the C++ implementation of PAW (Fortran), which was the plotting and I/O tool for CERN experiments. Later on, ROOT evolved in a much more complete and complex tool, adding new packages like PyROOT (finally!), which provided an (almost) fully functional Python interpreter which made ROOT much more user friendly. Its core is still in C++ and uses a debatable memory management method, which is however hidden from the common user.\n",
    "\n",
    "In spite of the many flaws, its I/O and the statistical analysis tools included are formidable.\n",
    "\n",
    "Installing ROOT is [not trivial](https://root.cern/install/).\n",
    "\n",
    "Fortunately, ROOT files can be opened with a non-ROOT library, [uproot](https://github.com/scikit-hep/uproot). See the [documentation](https://uproot.readthedocs.io/en/latest/basic.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://scikit-hep.org/uproot3/examples/Zmumu.root -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "events = uproot.open(\"data/Zmumu.root:events\") # pass file name : name of the TTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numpy array - default library uses 'awkward'\n",
    "e1 = events['E1'].array(library=\"np\")\n",
    "\n",
    "# \"pd\" returns pandas Series\n",
    "mass = events['M'].array(library=\"pd\")\n",
    "mass.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary (hexadecimal) files\n",
    "\n",
    "The raw output of sensors often consists of hexadecimal files. Information is packed in a well defined format (similarly to how floating point numbers are formatted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/9nu2i111if55135/data_000637.dat -P ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read and process hexadecimal files in python you need to use the `\"b\"` option of `open()` and progress along the file *with steps of defined length* (depending on the size of the words information is packed into)\n",
    "\n",
    "There are several tool to display and edit hex/bin files:\n",
    "- from command line, using the `hexdump` command:\n",
    "- online, using online tools, like the [hexed.it](https://hexed.it/) website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hexdump data/data_000637.dat # warning, the file is large\n",
    "# every line corresponds to a 64-bit word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example from data collected from an FPGA implementing a TDC (Time-to-Digital-Converter). Relevant infomation are the coordinates of the TDC channels and their time measurements, which are \"packed\" according to the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/data_format.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's convenient to use the Python `struct` to read the binary words: [documentation](https://docs.python.org/3/library/struct.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct, time\n",
    "\n",
    "data = {}\n",
    "\n",
    "with open('data/data_000637.dat', 'rb') as file:\n",
    "    file_content = file.read()\n",
    "    word_counter = 0\n",
    "    word_size = 8 # size of the word in bytes\n",
    "    for i in range(0, len(file_content), word_size):\n",
    "        word_counter += 1\n",
    "        if word_counter > 10: break\n",
    "        word = struct.unpack('<q', file_content[i : i + word_size])[0] # get an 8-byte word\n",
    "        head     = (word >> 62) & 0x3\n",
    "        fpga     = (word >> 58) & 0xF\n",
    "        tdc_chan = (word >> 49) & 0x1FF\n",
    "        orb_cnt  = (word >> 17) & 0xFFFFFFFF\n",
    "        bx       = (word >> 5 ) & 0xFFF\n",
    "        tdc_meas = (word >> 0 ) & 0x1F\n",
    "        #if i == 0: print ('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format('HEAD', 'FPGA', 'CHANNEL', 'ORBIT_CNT', 'BX_CNT', 'TDC_MEAS'))\n",
    "        #print('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(head, fpga, tdc_chan, orb_cnt, bx, tdc_meas))\n",
    "        entry = {'HEAD' : head, 'FPGA' : fpga, 'CHANNEL' : tdc_chan, 'ORBIT_CNT' : orb_cnt, 'BX_CNT' : bx, 'TDC_MEAS' : tdc_meas}\n",
    "        #df = df.append(entry, ignore_index=True)\n",
    "        data[word_counter] = entry\n",
    "        \n",
    "df = pd.DataFrame(data).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
